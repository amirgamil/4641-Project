{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Kaggle shape:  (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "cnx = sqlite3.connect(\"FPA_FOD_20170508.sqlite\")\n",
    "kaggle = pd.read_sql_query(\"SELECT * FROM Fires LIMIT 10000\", cnx)\n",
    "print(\"Original Kaggle shape: \", np.shape(kaggle))\n",
    "# print(\"Dimensions: \", kaggle.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New Kaggle shape (10000, 26)\n"
     ]
    }
   ],
   "source": [
    "# removing some columns\n",
    "kaggle = kaggle.drop(['Shape', 'FPA_ID'], axis=1)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "kaggle['SOURCE_SYSTEM_TYPE'] = kaggle[['SOURCE_SYSTEM_TYPE']].apply(le.fit_transform)\n",
    "kaggle[['SOURCE_SYSTEM']] = kaggle[['SOURCE_SYSTEM']].apply(le.fit_transform)\n",
    "kaggle[['NWCG_REPORTING_AGENCY']] = kaggle[['NWCG_REPORTING_AGENCY']].apply(le.fit_transform)\n",
    "kaggle[['NWCG_REPORTING_UNIT_ID']] = kaggle[['NWCG_REPORTING_UNIT_ID']].apply(le.fit_transform)\n",
    "kaggle[['NWCG_REPORTING_UNIT_NAME']] = kaggle[['NWCG_REPORTING_UNIT_NAME']].apply(le.fit_transform)\n",
    "kaggle[['SOURCE_REPORTING_UNIT_NAME']] = kaggle[['SOURCE_REPORTING_UNIT_NAME']].apply(le.fit_transform)\n",
    "kaggle[['FIRE_SIZE_CLASS']] = kaggle[['FIRE_SIZE_CLASS']].apply(le.fit_transform)\n",
    "kaggle[['OWNER_DESCR']] = kaggle[['OWNER_DESCR']].apply(le.fit_transform)\n",
    "kaggle[['STATE']] = kaggle[['STATE']].apply(le.fit_transform)\n",
    "kaggle[['FIPS_NAME']] = kaggle[['FIPS_NAME']].fillna(value=\"Unknown\").apply(le.fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "kaggle = kaggle.drop(['LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'STAT_CAUSE_CODE'], axis=1)\n",
    "\n",
    "kaggle = kaggle.drop(['SOURCE_REPORTING_UNIT'], axis=1)\n",
    "kaggle['COUNTY'] = pd.to_numeric(kaggle['COUNTY'])\n",
    "kaggle['FIPS_CODE'] = pd.to_numeric(kaggle['FIPS_CODE'])\n",
    "kaggle['DISCOVERY_TIME'] = pd.to_numeric(kaggle['DISCOVERY_TIME'])\n",
    "kaggle['CONT_TIME'] = pd.to_numeric(kaggle['CONT_TIME'])\n",
    "\n",
    "\n",
    "# Could not find 'STATE_CAUSE_DESCR' to drop\n",
    "print(\"New Kaggle shape\", np.shape(kaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New Kaggle shape (10000, 20)\nNew labels shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# print(kaggle['STAT_CAUSE_DESCR'])\n",
    "# kaggle.head(5)\n",
    "# convert the cause to numbers\n",
    "unique = np.unique(kaggle['STAT_CAUSE_DESCR'])\n",
    "desc2index = {v: k for k, v in enumerate(unique)}\n",
    "index2desc = {k: v for k, v in enumerate(desc2index)}\n",
    "kaggle['STAT_CAUSE_DESCR'] = kaggle['STAT_CAUSE_DESCR'].map(desc2index)\n",
    "#Remove any Nan or numbers with too large of a magnitude\n",
    "kaggle = kaggle.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "#Get the labels\n",
    "labels = np.array(kaggle['STAT_CAUSE_DESCR'])\n",
    "#drop the labels from the data\n",
    "kaggle= kaggle.drop('STAT_CAUSE_DESCR', axis = 1)\n",
    "# print(\"Dimensions: \", kaggle.columns.values.tolist())\n",
    "feature_list = list(kaggle.columns)\n",
    "kaggle = np.array(kaggle)\n",
    "print(\"New Kaggle shape\", np.shape(kaggle))\n",
    "print(\"New labels shape\", np.shape(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Features Shape: (7500, 20)\nTraining Labels Shape: (7500,)\nTesting Features Shape: (2500, 20)\nTesting Labels Shape: (2500,)\n"
     ]
    }
   ],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(kaggle, labels, test_size = 0.25)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average baseline error:  1.5\ncorrect:  1205 wrong:  1295\n---------\nAccuracy:  0.482\n"
     ]
    }
   ],
   "source": [
    "# The baseline predictions uses the mode of the labels\n",
    "counts = np.bincount(train_labels)\n",
    "mode = np.argmax(counts)\n",
    "baseline_preds = np.ones_like(test_labels) * mode\n",
    "# print(np.shape(baseline_preds), baseline_preds)\n",
    "# print(np.shape(test_labels), test_labels)\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - test_labels)\n",
    "correct = np.sum(baseline_preds == test_labels)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2))\n",
    "print(\"correct: \", correct, \"wrong: \", np.size(test_labels) - correct)\n",
    "print(\"---------\")\n",
    "print(\"Accuracy: \", correct / np.size(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "maxAccuracy = 0.0\n",
    "maxAccuracyDepth = 0.0\n",
    "maxAccuracyEstimators = 0.0\n",
    "maxAccuracyFeatures = 0.0\n",
    "for i in range(0, 1001, 5): \n",
    "    if (i == 0):\n",
    "        continue\n",
    "    for j in range (1, 51, 1):\n",
    "        for k in range(1, 10, 1):\n",
    "            rf = RandomForestRegressor(n_estimators = j, max_depth=i, max_features=0.1 * k)\n",
    "            # Train the model on training data\n",
    "            rf.fit(train_features, train_labels)\n",
    "            # Use the forest's predict method on the test data\n",
    "            predictions = rf.predict(test_features)\n",
    "            predictions = np.rint(predictions)\n",
    "            # print(predictions[: 20], \"...\")\n",
    "            # print(\"---------\")\n",
    "            # print(test_labels[: 20], \"...\")\n",
    "            # Calculate the absolute errors\n",
    "            correct = np.sum(predictions == test_labels)\n",
    "            accuracy = correct / np.size(test_labels)\n",
    "            # print(\"max depth \", i, \"max estimators\", j)\n",
    "            # print(\"correct: \", correct, \"wrong: \", np.size(test_labels) - correct)\n",
    "            # print(\"Accuracy: \", accuracy)\n",
    "            # print(\"---------\")\n",
    "            if (accuracy > maxAccuracy):\n",
    "                maxAccuracy = accuracy\n",
    "                maxAccuracyDepth = i\n",
    "                maxAccuracyEstimators = j\n",
    "                maxAccuracyFeatures = k\n",
    "            # Print out the mean absolute error (mae)\n",
    "            # print('Mean Absolute Error:', np.mean(errors))\n",
    "    print (\"Max accuracy of \", maxAccuracy, \" given by max_depth of\", maxAccuracyDepth, \", n_estimators of \", maxAccuracyEstimators, \", and max_features of \", maxAccuracyFeatures)\n",
    "print(\"-----------\")\n",
    "print (\"Max accuracy of \", maxAccuracy, \" given by max_depth of\", maxAccuracyDepth, \", n_estimators of \", maxAccuracyEstimators, \", and max_features of \", maxAccuracyFeatures)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=20, max_features=0.7, n_estimators=2)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with n decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 2, max_depth=20, max_features=0.7)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5. 5. 6. 4. 6. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 2. 3. 6. 5. 3.] ...\n---------\n[5 5 4 6 5 5 5 5 5 1 5 1 5 5 5 6 4 1 5 0] ...\ncorrect:  1226 wrong:  1274\n---------\nAccuracy:  0.4904\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "predictions = np.rint(predictions)\n",
    "print(predictions[: 20], \"...\")\n",
    "print(\"---------\")\n",
    "print(test_labels[: 20], \"...\")\n",
    "# Calculate the absolute errors\n",
    "correct = np.sum(predictions == test_labels)\n",
    "print(\"correct: \", correct, \"wrong: \", np.size(test_labels) - correct)\n",
    "print(\"---------\")\n",
    "print(\"Accuracy: \", correct / np.size(test_labels))\n",
    "# Print out the mean absolute error (mae)\n",
    "# print('Mean Absolute Error:', np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the forest's predict method on the test data\n",
    "# predictions = rf.predict(test_features)\n",
    "# predictions = np.rint(predictions)\n",
    "# print(predictions[: 20], \"...\")\n",
    "# print(\"---------\")\n",
    "# print(test_labels[: 20], \"...\")\n",
    "# # Calculate the absolute errors\n",
    "# errors = abs(predictions - test_labels)\n",
    "# print(errors)\n",
    "# # Print out the mean absolute error (mae)\n",
    "# print('Mean Absolute Error:', np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate mean absolute percentage error (MAPE)\n",
    "# mape = 100 * ((errors+1)/ (test_labels + 1))\n",
    "# print(mape)\n",
    "# print(np.mean(mape))\n",
    "# # Calculate and display accuracy\n",
    "# accuracy = 100 - np.mean(mape)\n",
    "# print('Accuracy:', accuracy, '%.')"
   ]
  }
 ]
}